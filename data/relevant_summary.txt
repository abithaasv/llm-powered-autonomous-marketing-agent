AI systems are being tricked by indirect prompt injection attacks, which are considered one of the most concerning ways that language models could be abused by hackers. As generative AI systems are put to work by big corporations and smaller startups, the cybersecurity industry is scrambling to raise awareness of the potential dangers. There isn�t one magic fix, but common security practices can reduce the risks. Indirect prompt injection attacks involve hidden instructions in data that the LLM processes, and the fundamental risk is that whoever provides input to the LLM has a high degree of influence over the output. Security researchers have demonstrated how indirect prompt injections could be used to steal data, manipulate someone�s r�sum�, and run code remotely on a machine. Companies should use a series of security industry best practices to reduce the risks, such as siloing systems, following the principle of least privileges, and using specially trained models to identify known malicious inputs and unsafe outputs.